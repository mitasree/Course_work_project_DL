# Deep Learning Coursework – IIT Madras (2025)

This repository contains my work from the **Deep Learning course at IIT Madras (2025)**.  
The assignments focused on implementing and analyzing different sequence models and activation functions using deep learning techniques.  

## ● Project Overview
As part of the course assignments, I implemented and compared various neural network architectures for sequence modeling and text-based tasks.  
The key components include:  

- **Encoder–Decoder Architecture**  
- **LSTM (Long Short-Term Memory) Networks**  
- **GRU (Gated Recurrent Unit) Networks**  
- **Comparison of Activation Functions** (ReLU, Tanh, Sigmoid, etc.)  

## ● Technologies Used
- Python  
- NumPy, Pandas  
- TensorFlow  
- Matplotlib, Seaborn (for visualizations)  

## ● Key Learnings
- Understood the working principles of encoder–decoder models for sequence-to-sequence learning.  
- Explored differences between **LSTM** and **GRU** in terms of performance and computational efficiency.  
- Compared the impact of different **activation functions** on training dynamics and model convergence.  

## ● Future Extensions
- Experiment with **attention mechanisms** on top of encoder–decoder models.  
- Apply these models to real-world NLP datasets beyond coursework.  
- Deploy trained models for demonstration as a simple web app.  

## ● Acknowledgment
This project was completed as part of the **Deep Learning coursework at IIT Madras (2025)** under faculty guidance.  

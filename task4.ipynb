{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc9a9c2a-b6af-4470-b6ca-838c5c50c671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from math import ceil\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm        # Progress bar library for Jupyter Notebook\n",
    "\n",
    "# Deep learning framework for building and training models\n",
    "import tensorflow as tf\n",
    "## Pre-trained model for image feature extraction\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "## Tokenizer class for captions tokenization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "## Function for padding sequences to a specific length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "## Class for defining Keras models\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, concatenate, Bidirectional, Dot, Activation, RepeatVector, Multiply, Lambda\n",
    "\n",
    "# For checking score\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0e3e44f-221d-4360-89b5-4e9803976360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the input and output directory\n",
    "INPUT_DIR = './'\n",
    "OUTPUT_DIR = './image_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70e62b04-c6cb-4c25-98c7-a5a9ed36676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# We are going to use pretraind vgg model\n",
    "# Load the vgg16 model\n",
    "model = VGG16()\n",
    "\n",
    "# Restructuring the model to remove the last classification layer, this will give us access to the output features of the model\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\n",
    "# Printing the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0504d1b0-d3fe-4a95-8cc9-fc8e7327253f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd73e951177040499aa2452f13cf84bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize an empty dictionary to store image features\n",
    "image_features = {}\n",
    "\n",
    "# Define the directory path where images are located\n",
    "img_dir = os.path.join(INPUT_DIR, 'Images')\n",
    "\n",
    "# Loop through each image in the directory\n",
    "for img_name in tqdm(os.listdir(img_dir)):\n",
    "    # Load the image from file\n",
    "    img_path = os.path.join(img_dir, img_name)\n",
    "    image = load_img(img_path, target_size=(224, 224))\n",
    "    # Convert image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    # Reshape the data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # Preprocess the image for ResNet50\n",
    "    image = preprocess_input(image)\n",
    "    # Extract features using the pre-trained ResNet50 model\n",
    "    image_feature = model.predict(image, verbose=0)\n",
    "    # Get the image ID by removing the file extension\n",
    "    image_id = img_name.split('.')[0]\n",
    "    # Store the extracted feature in the dictionary with the image ID as the key\n",
    "    image_features[image_id] = image_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeb836d6-082a-497d-b1a7-b957aacb10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the image features in pickle\n",
    "pickle.dump(image_features, open(os.path.join(OUTPUT_DIR, 'image_feature.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fe423ee-b0af-4518-88ea-b6e25d08643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features from pickle file\n",
    "pickle_file_path = os.path.join(OUTPUT_DIR, 'image_feature.pkl')\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    loaded_features = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86e816c2-e6ca-475d-892b-dc35901a3055",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(INPUT_DIR, 'captions.txt'), 'r') as file:\n",
    "    next(file)\n",
    "    captions_doc = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2276127f-3adf-4fe2-a374-3df3dd345117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780c45d0c3624c188955c29c0cbd0924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of captions: 2447\n"
     ]
    }
   ],
   "source": [
    "# Create mapping of image to captions\n",
    "image_to_captions_mapping = defaultdict(list)\n",
    "\n",
    "# Process lines from captions_doc\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    # Split the line by comma(,)\n",
    "    tokens = line.split(',')\n",
    "    if len(tokens) < 2:\n",
    "        continue\n",
    "    image_id, *captions = tokens\n",
    "    # Remove extension from image ID\n",
    "    image_id = image_id.split('.')[0]\n",
    "    # Convert captions list to string\n",
    "    caption = \" \".join(captions)\n",
    "    # Store the caption using defaultdict\n",
    "    image_to_captions_mapping[image_id].append(caption)\n",
    "\n",
    "# Print the total number of captions\n",
    "total_captions = sum(len(captions) for captions in image_to_captions_mapping.values())\n",
    "print(\"Total number of captions:\", total_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4cb8fbc7-c85a-44c2-bc58-76516f232622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for processing the captions\n",
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            # Take one caption at a time\n",
    "            caption = captions[i]\n",
    "            # Preprocessing steps\n",
    "            # Convert to lowercase\n",
    "            caption = caption.lower()\n",
    "            # Remove non-alphabetical characters\n",
    "            caption = ''.join(char for char in caption if char.isalpha() or char.isspace())\n",
    "            # Remove extra spaces\n",
    "            caption = caption.replace('\\s+', ' ')\n",
    "            # Add unique start and end tokens to the caption\n",
    "            caption = 'startseq ' + ' '.join([word for word in caption.split() if len(word) > 1]) + ' endseq'\n",
    "            captions[i] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88140918-74e2-4ba4-862c-f63ccd4c4e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a List of All Captions\n",
    "all_captions = [caption for captions in image_to_captions_mapping.values() for caption in captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d2cebf6e-8735-4b8d-be54-b41b4b06755c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' a red ball next to it .',\n",
       " ' stands on shore shaking off water',\n",
       " ' facing the water and the city skyline .',\n",
       " ' being held by the male   sitting next to a pond with a near by stroller .',\n",
       " ' with stick .',\n",
       " ' standing on an ice   looking into something covered with a blue tarp .',\n",
       " ' man in background watches .',\n",
       " ' as seen from behind her .',\n",
       " ' one with a yellow and orange ball   play in some water in front of a field .',\n",
       " ' as seen from behind .']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_captions[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8e942092-2b17-4130-92e4-52a56de40892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the Text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c2dccc6e-fe5a-454d-ac64-81700119be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file)\n",
    "\n",
    "# Load the tokenizer\n",
    "with open('tokenizer.pkl', 'rb') as tokenizer_file:\n",
    "    tokenizer = pickle.load(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d673f93-c97b-4f3d-9dc1-c1414c05fbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 2102\n",
      "Maximum Caption Length: 24\n"
     ]
    }
   ],
   "source": [
    "# Calculate maximum caption length\n",
    "max_caption_length = max(len(tokenizer.texts_to_sequences([caption])[0]) for caption in all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Print the results\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "print(\"Maximum Caption Length:\", max_caption_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4258f16e-62d1-4955-a75f-0249a1eca3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a List of Image IDs\n",
    "image_ids = list(image_to_captions_mapping.keys())\n",
    "# Splitting into Training and Test Sets\n",
    "split = int(len(image_ids) * 0.90)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ee55550-92eb-4d46-ae42-af9194cb14c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator function\n",
    "def data_generator(data_keys, image_to_captions_mapping, features, tokenizer, max_caption_length, vocab_size, batch_size):\n",
    "    # Lists to store batch data\n",
    "    X1_batch, X2_batch, y_batch = [], [], []\n",
    "    # Counter for the current batch size\n",
    "    batch_count = 0\n",
    "\n",
    "    while True:\n",
    "        # Loop through each image in the current batch\n",
    "        for image_id in data_keys: \n",
    "            # Get the captions associated with the current image\n",
    "            captions = image_to_captions_mapping[image_id]\n",
    "\n",
    "            # Loopyi through each caption for the current image\n",
    "            for caption in captions:\n",
    "                # Convert the caption to a sequence of token IDs\n",
    "                caption_seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "\n",
    "                # Loop through the tokens in the caption sequence\n",
    "                for i in range(1, len(caption_seq)):\n",
    "                    # Split the sequence into input and output pairs\n",
    "                    in_seq, out_seq = caption_seq[:i], caption_seq[i]\n",
    "\n",
    "                    # Pad the input sequence to the specified maximum caption length\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_caption_length)[0]\n",
    "\n",
    "                    # Convert the output sequence to one-hot encoded format\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\n",
    "                    # Append data to batch lists\n",
    "                    X1_batch.append(features[image_id][0])  # Image features\n",
    "                    X2_batch.append(in_seq)  # Input sequence\n",
    "                    y_batch.append(out_seq)  # Output sequence\n",
    "\n",
    "                    # Increase the batch counter\n",
    "                    batch_count += 1\n",
    "\n",
    "                    # If the batch is complete, yield the batch and reset lists and counter\n",
    "                    if batch_count == batch_size:\n",
    "                        X1_batch, X2_batch, y_batch = np.array(X1_batch), np.array(X2_batch), np.array(y_batch)\n",
    "                        yield [X1_batch, X2_batch], y_batch\n",
    "                        X1_batch, X2_batch, y_batch = [], [], []\n",
    "                        batch_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "939643ba-9b82-4150-a489-284d402a7dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 11:37:46.029703: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-04-04 11:37:46.030833: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-04-04 11:37:46.031739: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2025-04-04 11:37:46.102690: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2025-04-04 11:37:46.128165: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-04-04 11:37:46.129117: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-04-04 11:37:46.129890: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Embedding, RepeatVector, Bidirectional, Lambda, concatenate, Activation, Dot\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Load VGG16 as the encoder\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False)  # Exclude top FC layers\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False  # Freeze VGG16 weights\n",
    "\n",
    "# Encoder Model\n",
    "inputs1 = Input(shape=(224, 224, 3))  # Image input size\n",
    "fe1 = vgg_model(inputs1)  # Extract features\n",
    "fe1 = tf.keras.layers.GlobalAveragePooling2D()(fe1)  # Convert to (None, 512)\n",
    "fe2 = Dense(50, activation='relu')(fe1)  # Reduce to 50 hidden units\n",
    "fe2_projected = RepeatVector(max_caption_length)(fe2)  # Expand for sequence processing\n",
    "fe2_projected = Bidirectional(LSTM(50, return_sequences=True))(fe2_projected)\n",
    "\n",
    "# Sequence feature layers (Decoder)\n",
    "inputs2 = Input(shape=(max_caption_length,))\n",
    "se1 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)  # Use hidden size 50\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = Bidirectional(LSTM(50, return_sequences=True))(se2)  # Reduce LSTM size to 50\n",
    "\n",
    "# Apply attention mechanism using Dot product\n",
    "attention = Dot(axes=[2, 2])([fe2_projected, se3])  # Calculate attention scores\n",
    "attention_scores = Activation('softmax')(attention)  # Softmax attention\n",
    "\n",
    "# Apply attention scores to sequence embeddings\n",
    "attention_context = Lambda(lambda x: tf.einsum('ijk,ijl->ikl', x[0], x[1]))([attention_scores, se3])\n",
    "\n",
    "# Sum the attended sequence embeddings along the time axis\n",
    "context_vector = tf.reduce_sum(attention_context, axis=1)\n",
    "\n",
    "# Decoder model\n",
    "decoder_input = concatenate([context_vector, fe2], axis=-1)\n",
    "decoder1 = Dense(50, activation='relu')(decoder_input)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder1)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Visualize the model\n",
    "plot_model(model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca74f400-90ae-4ede-9dd5-f6b2b15adb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 11:32:04.687580: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-04-04 11:32:04.690993: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-04-04 11:32:04.692684: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2025-04-04 11:32:04.881222: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2025-04-04 11:32:04.958016: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-04-04 11:32:04.960620: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-04-04 11:32:04.962925: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, Dense, RepeatVector, LSTM, Embedding, Bidirectional, Dot, Activation, Lambda, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Encoder model\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(50, activation='relu')(fe1)  # Use 50 units instead of 256\n",
    "fe2_projected = RepeatVector(max_caption_length)(fe2)\n",
    "fe2_projected = Bidirectional(LSTM(50, return_sequences=True))(fe2_projected)  # Adjusted to 50\n",
    "\n",
    "# Sequence feature layers\n",
    "inputs2 = Input(shape=(max_caption_length,))\n",
    "se1 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)  # Embedding should match\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = Bidirectional(LSTM(50, return_sequences=True))(se2)  # Adjusted to 50\n",
    "\n",
    "# Apply attention mechanism using Dot product\n",
    "attention = Dot(axes=[2, 2])([fe2_projected, se3])  # Calculate attention scores\n",
    "\n",
    "# Softmax attention scores\n",
    "attention_scores = Activation('softmax')(attention)\n",
    "\n",
    "# Apply attention scores to sequence embeddings\n",
    "attention_context = Lambda(lambda x: tf.einsum('ijk,ijl->ikl', x[0], x[1]))([attention_scores, se3])\n",
    "\n",
    "# Sum the attended sequence embeddings along the time axis\n",
    "context_vector = tf.reduce_sum(attention_context, axis=1)  # Shape: (batch_size, 100)\n",
    "\n",
    "# Decoder model\n",
    "decoder_input = concatenate([context_vector, fe2], axis=-1)  # Ensure matching shapes\n",
    "decoder1 = Dense(50, activation='relu')(decoder_input)  # Adjusted to 50\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder1)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Visualize the model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5f6789f4-0677-4964-97fd-aa548905b740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-03 22:13:44.344507: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-04-03 22:13:44.345329: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-04-03 22:13:44.346152: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2025-04-03 22:13:44.407589: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2025-04-03 22:13:44.428516: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2025-04-03 22:13:44.429145: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2025-04-03 22:13:44.429699: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# Encoder model\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "fe2_projected = RepeatVector(max_caption_length)(fe2)\n",
    "fe2_projected = Bidirectional(LSTM(256, return_sequences=True))(fe2_projected)\n",
    "\n",
    "# Sequence feature layers\n",
    "inputs2 = Input(shape=(max_caption_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = Bidirectional(LSTM(256, return_sequences=True))(se2)\n",
    "\n",
    "# Apply attention mechanism using Dot product\n",
    "attention = Dot(axes=[2, 2])([fe2_projected, se3])  # Calculate attention scores\n",
    "\n",
    "# Softmax attention scores\n",
    "attention_scores = Activation('softmax')(attention)\n",
    "\n",
    "# Apply attention scores to sequence embeddings\n",
    "attention_context = Lambda(lambda x: tf.einsum('ijk,ijl->ikl', x[0], x[1]))([attention_scores, se3])\n",
    "\n",
    "# Sum the attended sequence embeddings along the time axis\n",
    "context_vector = tf.reduce_sum(attention_context, axis=1)\n",
    "\n",
    "# Decoder model\n",
    "decoder_input = concatenate([context_vector, fe2], axis=-1)\n",
    "decoder1 = Dense(256, activation='relu')(decoder_input)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder1)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Visualize the model\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "10c16aa1-3305-4f35-8743-382d31ecc971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the number of epochs, batch size\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "\n",
    "# Calculate the steps_per_epoch based on the number of batches in one epoch\n",
    "steps_per_epoch = ceil(len(train) / batch_size)\n",
    "validation_steps = ceil(len(test) / batch_size)  # Calculate the steps for validation data\n",
    "\n",
    "def data_generator(dataset, image_to_captions_mapping, loaded_features, tokenizer, max_caption_length, vocab_size, batch_size):\n",
    "    while True:  # Infinite loop for generator\n",
    "        for img_id, captions in enumerate(dataset):  # Assuming dataset is a list\n",
    "            feature = loaded_features.get(str(img_id), None)  # Convert img_id to string if necessary\n",
    "  # Extract image features\n",
    "            feature = np.expand_dims(feature, axis=0)  # Add batch dimension\n",
    "\n",
    "            for caption in captions:\n",
    "                # Convert caption into sequence\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                \n",
    "                for i in range(1, len(seq)):\n",
    "                    input_seq, output_word = seq[:i], seq[i]\n",
    "                    \n",
    "                    # Pad input sequence\n",
    "                    input_seq = pad_sequences([input_seq], maxlen=max_caption_length, padding='post')[0]\n",
    "                    \n",
    "                    # Convert output word to one-hot vector\n",
    "                    output_word = to_categorical([output_word], num_classes=vocab_size)[0]\n",
    "                    \n",
    "                    yield ([feature, input_seq], output_word)  # Return as tuple\n",
    "\n",
    "       \n",
    "# Loop through the epochs for training\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # Set up data generators\n",
    "    train_generator = data_generator(train, image_to_captions_mapping, loaded_features, tokenizer, max_caption_length, vocab_size, batch_size)\n",
    "    test_generator = data_generator(test, image_to_captions_mapping, loaded_features, tokenizer, max_caption_length, vocab_size, batch_size)\n",
    "    \n",
    "    model.fit(train_generator, epochs=1, steps_per_epoch=steps_per_epoch,\n",
    "          validation_data=test_generator, validation_steps=validation_steps,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0dfa280c-f0be-49f8-95b1-8b02702e6824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys in loaded_features: ['439037721_cdf1fc7358', '3571193625_835da90c5e', '542269487_5d77b363eb', '3057497487_57ecc60ff1', '441398149_297146e38d', '3430526230_234b3550f6', '2181724497_dbb7fcb0a9', '162743064_bb242faa31', '2754271176_4a2cda8c15', '3282897060_8c584e2ce8']\n"
     ]
    }
   ],
   "source": [
    "print(\"Available keys in loaded_features:\", list(loaded_features.keys())[:10])  # Print first 10 keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "326266f9-a6f9-44a2-a6e2-1b68ca46830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(OUTPUT_DIR+'/mymodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e5c96f8-182d-47bc-a51e-4d36f5d10ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_from_index(index, tokenizer):\n",
    "    return next((word for word, idx in tokenizer.word_index.items() if idx == index), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0a9626eb-afe7-4ca5-a181-dca8e04b6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(model, image_features, tokenizer, max_caption_length):\n",
    "    # Initialize the caption sequence\n",
    "    caption = 'startseq'\n",
    "    \n",
    "    # Generate the caption\n",
    "    for _ in range(max_caption_length):\n",
    "        # Convert the current caption to a sequence of token indices\n",
    "        sequence = tokenizer.texts_to_sequences([caption])[0]\n",
    "        # Pad the sequence to match the maximum caption length\n",
    "        sequence = pad_sequences([sequence], maxlen=max_caption_length)\n",
    "        # Predict the next word's probability distribution\n",
    "        yhat = model.predict([image_features, sequence], verbose=0)\n",
    "        # Get the index with the highest probability\n",
    "        predicted_index = np.argmax(yhat)\n",
    "        # Convert the index to a word\n",
    "        predicted_word = get_word_from_index(predicted_index, tokenizer)\n",
    "        \n",
    "        # Append the predicted word to the caption\n",
    "        caption += \" \" + predicted_word\n",
    "        \n",
    "        # Stop if the word is None or if the end sequence tag is encountered\n",
    "        if predicted_word is None or predicted_word == 'endseq':\n",
    "            break\n",
    "    \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c5e0e563-7928-48f9-ada9-fcd4fdaa9008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3fd807ce3e482a9fb2ac9caaa63157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/209 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize lists to store actual and predicted captions\n",
    "actual_captions_list = []\n",
    "predicted_captions_list = []\n",
    "\n",
    "# Loop through the test data\n",
    "for key in tqdm(test):\n",
    "    # Get actual captions for the current image\n",
    "    actual_captions = image_to_captions_mapping[key]\n",
    "    # Predict the caption for the image using the model\n",
    "    predicted_caption = predict_caption(model, loaded_features[key], tokenizer, max_caption_length)\n",
    "    \n",
    "    # Split actual captions into words\n",
    "    actual_captions_words = [caption.split() for caption in actual_captions]\n",
    "    # Split predicted caption into words\n",
    "    predicted_caption_words = predicted_caption.split()\n",
    "    \n",
    "    # Append to the lists\n",
    "    actual_captions_list.append(actual_captions_words)\n",
    "    predicted_captions_list.append(predicted_caption_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "783a41e3-cb78-48bf-935e-036e55edba44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0830769230769229\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "bleu_scores = []\n",
    "\n",
    "for ref, hyp in zip(actual_captions_list, predicted_captions_list):\n",
    "    try:\n",
    "        score = sentence_bleu(ref, hyp, weights=(1.0, 0, 0, 0))  # BLEU-1\n",
    "        bleu_scores.append(score)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# Compute and print only the mean BLEU-1 score\n",
    "mean_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
    "print(mean_bleu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "666b32e6-e08e-4087-9ecb-9181e69db7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.0830769230769229\n",
      "BLEU-2: 0.0167485232540874\n",
      "BLEU-3: 0.00041095688632418085\n",
      "BLEU-4: 9.111198281682361e-81\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "bleu1_scores, bleu2_scores, bleu3_scores, bleu4_scores = [], [], [], []\n",
    "\n",
    "for ref, hyp in zip(actual_captions_list, predicted_captions_list):\n",
    "    try:\n",
    "        bleu1_scores.append(sentence_bleu(ref, hyp, weights=(1.0, 0, 0, 0)))  # BLEU-1\n",
    "        bleu2_scores.append(sentence_bleu(ref, hyp, weights=(0.5, 0.5, 0, 0)))  # BLEU-2\n",
    "        bleu3_scores.append(sentence_bleu(ref, hyp, weights=(0.33, 0.33, 0.33, 0)))  # BLEU-3\n",
    "        bleu4_scores.append(sentence_bleu(ref, hyp, weights=(0.25, 0.25, 0.25, 0.25)))  # BLEU-4\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# Compute mean BLEU scores\n",
    "mean_bleu1 = sum(bleu1_scores) / len(bleu1_scores) if bleu1_scores else 0\n",
    "mean_bleu2 = sum(bleu2_scores) / len(bleu2_scores) if bleu2_scores else 0\n",
    "mean_bleu3 = sum(bleu3_scores) / len(bleu3_scores) if bleu3_scores else 0\n",
    "mean_bleu4 = sum(bleu4_scores) / len(bleu4_scores) if bleu4_scores else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"BLEU-1: {mean_bleu1}\")\n",
    "print(f\"BLEU-2: {mean_bleu2}\")\n",
    "print(f\"BLEU-3: {mean_bleu3}\")\n",
    "print(f\"BLEU-4: {mean_bleu4}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060bbd2b-a60b-4884-8679-47e65d9a4408",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m predicted_captions_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Loop through the test data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mtest\u001b[49m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Get actual captions for the current image\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     actual_captions \u001b[38;5;241m=\u001b[39m image_to_captions_mapping[key]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Predict the caption for the image using the model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Initialize lists to store actual and predicted captions\n",
    "actual_captions_list = []\n",
    "predicted_captions_list = []\n",
    "\n",
    "# Loop through the test data\n",
    "for key in tqdm(test):\n",
    "    # Get actual captions for the current image\n",
    "    actual_captions = image_to_captions_mapping[key]\n",
    "    # Predict the caption for the image using the model\n",
    "    predicted_caption = predict_caption(model, loaded_features[key], tokenizer, max_caption_length)\n",
    "    \n",
    "    # Split actual captions into words\n",
    "    actual_captions_words = [caption.split() for caption in actual_captions]\n",
    "    # Split predicted caption into words\n",
    "    predicted_caption_words = predicted_caption.split()\n",
    "    \n",
    "    # Append to the lists\n",
    "    actual_captions_list.append(actual_captions_words)\n",
    "    predicted_captions_list.append(predicted_caption_words)\n",
    "\n",
    "print(\"Actual vs Predicted Captions:\\n\")\n",
    "\n",
    "for idx, (actual, predicted) in enumerate(zip(actual_captions_list, predicted_captions_list)):\n",
    "    # Actual is a list of lists (multiple ground truths)\n",
    "    actual_sentences = [' '.join(words) for words in actual]\n",
    "    predicted_sentence = ' '.join(predicted)\n",
    "    print(f\"Image {idx+1}:\")\n",
    "    print(f\"  Actual: {actual_sentences}\")\n",
    "    print(f\"  Predicted: {predicted_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0e705-08fc-4c81-941c-b169aae0d0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
